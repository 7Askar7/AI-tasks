- [X] **Нейрость и ее разбор:**
      
   ```python
  class NeuralClassification(nn.Module):
    def __init__(self):
        super(NeuralClassification, self).__init__()
        self.embedding = nn.Embedding(30522, 768)
        self.conv1 = nn.Conv1d(768, 512, kernel_size=3)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(512, 2)  # Изменение размерности входа на 512

    def forward(self, x):
        embedded = self.embedding(x)
        embedded = embedded.squeeze(1).permute(0, 2, 1)  # Удаление дополнительного измерения и перестановка размерностей
        conv = self.conv1(embedded)
        conv = self.dropout(conv)
        conv = F.relu(conv)
        conv = torch.max(conv, dim=2)[0]  # Применение max pooling по временной размерности
        output = self.fc(conv)
        return output
   ```
- [X] **Рассмотрим NeuralClassification.**

* ```python
   self.embedding = nn.Embedding(30522, 768)
   ```
   - Здесь определяется слой Embedding, который используется для преобразования индексов слов в плотные векторные 
   представления размерности 768. 30522 - это размер словаря, а 768 - размерность векторных представлений.


* ```python
   self.conv1 = nn.Conv1d(768, 512, kernel_size=3)
   ```
   - Это слой свертки (Conv1d), который применяет свертку на входных данных размерности 768 с ядром размера 3 
   и создает 512 фильтров (или признаков) на выходе. Свертка позволяет извлекать локальные признаки из входных данных.


* ```python
   self.dropout = nn.Dropout(0.2) 
   ```
   - Это слой регуляризации Dropout, который применяет случайное обнуление 
   входных элементов с вероятностью 0.2. Он помогает предотвратить переобучение модели 
   и улучшить ее обобщающую способность.


* ```python
   self.fc = nn.Linear(512, 2)
   ```
   - Это полносвязный слой (Linear), который принимает входные данные размерности 512 
   и преобразует их в выходные данные размерности 2. В данном случае, модель предсказывает 2 класса.


- [X] **В методе forward модели, происходит последовательное применение каждого слоя:**

* ```python
   embedded = self.embedding(x)
   ```
   - Входные данные x, которые представляют собой индексы слов, проходят через слой Embedding, 
   чтобы получить их векторные представления embedded.

* ```python
   embedded = embedded.squeeze(1).permute(0, 2, 1)
   ```
   - Удаляется дополнительное измерение, которое было добавлено в результате работы слоя Embedding, 
   и происходит перестановка размерностей, чтобы размерности соответствовали ожидаемому 
   формату для сверточного слоя. В результате получается 
   embedded с размерностью [batch_size, embedding_dim, sequence_length].

* ```python
   conv = self.conv1(embedded)
   ```
   - Векторные представления embedded проходят через сверточный слой conv1. 
   Результатом является тензор conv с 
   размерностью [batch_size, num_filters, output_length], 
   где num_filters - количество фильтров, а output_length - длина выходной последовательности.

* ```python
   conv = self.dropout(conv)
   ```
   - Применяется слой Dropout для случайного обнуления элементов в тензоре conv. 
   Это помогает в борьбе с переобучением модели
- [X] Почему мы не использовали LSTM или GRU:
      
   - Слой LSTM является очень "тяжелым" из соображений уменьшения времени обучения модели было решено не использовать данный слой.
   - Слой GRU легче чем LSTM, но все же занимает памят и увеличит время обучения.
   - Без данных слоёв модель показала отличный результат в **0.95**, а скорость обучения составила примерно **30 минут** !
